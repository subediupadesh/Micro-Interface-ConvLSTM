{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d2f0dc-04af-4f16-9652-651399b54814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from skimage import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65431617-01f9-46cb-a969-0c3bc2ba5e48",
   "metadata": {},
   "source": [
    "#### Loading Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd92f7-dc2c-4ca5-bff9-4f517326632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(\"..\")\n",
    "\n",
    "X_val = np.load(path+\"/train_test_data/X_val.npy\")\n",
    "y_val = np.load (path+\"/train_test_data/y_val.npy\")\n",
    "\n",
    "## New validation model data from new simulation to validate the model\n",
    "# X_val = np.load(path+\"/quantification/validation_dataset/X.npy\")\n",
    "# y_val = np.load (path+\"/quantification/validation_dataset/y.npy\")\n",
    "\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b0ed6-3796-4a95-b5fa-decb4339c069",
   "metadata": {},
   "source": [
    "#### Loading Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf42e0-2dfa-44e0-9366-c3ff2461b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path+\"/lstm_model_2.h5\")\n",
    "history = np.load(path+\"/history_2.npy\", allow_pickle=True).item()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74553d0a-892f-4fbe-94ea-48452986a406",
   "metadata": {},
   "source": [
    "#### Creating function for scaling pixel from 0 to 255 (Needed for Simalirity Index Measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc85bc-2464-4182-b8ef-d43e84a49ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling the array values from 0 to 255 for 0 as black pixel and 255 as white pixel for calculation of structural similarity\n",
    "## Scaling is done to both Ground Truth and Predicted Frame\n",
    "def scale_array(arr):\n",
    "    arr_min = np.min(arr)  ## Flattens the array and finds the min value inside it\n",
    "    arr_max = np.max(arr)  ## Flattens the array and finds the max value inside it\n",
    "    scaled_array = (arr - arr_min) / (arr_max - arr_min) * 255\n",
    "    scaled_array = scaled_array.astype(np.uint8)\n",
    "    \n",
    "    return scaled_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f9e11-7bd6-4de8-8a77-86b901c07a43",
   "metadata": {},
   "source": [
    "# GEN Zero  [3 PF$_{frames}$ --> 1 PD$^{G_0}$$_{frame}$] \n",
    "(All 3 PF$_{frames}$ are from Phase Field Simulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1fe75-8deb-46fb-9a7a-6b6b6cc50675",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6fc49-9f2f-4a35-9cf4-7c8cdf6299f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_array = y_val\n",
    "a, b, c, d, e = y_val.shape[0], y_val.shape[1], y_val.shape[2], y_val.shape[3], y_val.shape[4] \n",
    "PD_array = np.zeros((a, b, c, d, e))   ## Predicted arrays\n",
    "\n",
    "for i in range(X_val.shape[0]):\n",
    "  frames = X_val[i]   ## taking 1 validation video sample at one time\n",
    "  PD_array[i] = model.predict(np.expand_dims(frames, axis=0), verbose=0, workers=10, use_multiprocessing=True)\n",
    "\n",
    "GT_array.shape, PD_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49fb1a-d12b-4e58-838e-49b67d9d2261",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d6124-413d-4604-86d9-a7f35c8d8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = 10 # Video frame to look for visualtization\n",
    "\n",
    "fig, axes = plt.subplots(2, 17, figsize=(28, 4))\n",
    "fig.suptitle(f\"Generation 1 Prediction of Video Sequence No: {vs}\" , fontsize=20)\n",
    "for idx, ax in enumerate(axes[0]):\n",
    "    ax.imshow(np.squeeze(GT_array[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"GT Frame {idx + 4}\", size=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for idx, ax in enumerate(axes[1]):\n",
    "    ax.imshow(np.squeeze(PD_array[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"Pred Frame {idx + 4}\", size=8)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065991a-6272-4623-be11-69cf681c9bf1",
   "metadata": {},
   "source": [
    "### Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d94fb1-6902-41b7-bd0e-0078ab8ab898",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling the array values from 0 to 255 for 0 as black pixel and 255 as white pixel for calculation of structural similarity\n",
    "## Scaling is done to both Ground Truth and Predicted Frame\n",
    "\n",
    "GT_scaled = scale_array(y_val)\n",
    "PD_scaled = scale_array(PD_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d1ccf-a8c3-408d-9832-8a3f0f0e33bd",
   "metadata": {},
   "source": [
    "#### For Individual Image Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e9519-54c6-4463-b5e3-0c77d8581595",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Individual Image Frames\n",
    "mse = np.zeros((a,b))\n",
    "rmse = np.zeros((a,b))\n",
    "psnr = np.zeros((a,b))\n",
    "ssim = np.zeros((a,b))\n",
    "\n",
    "for i in range(GT_scaled.shape[0]):\n",
    "  for j in range(GT_scaled.shape[1]):\n",
    "    mse[i][j] = metrics.mean_squared_error(GT_scaled[i,j,:,:,0], PD_scaled[i,j,:,:,0])\n",
    "    rmse[i][j] = metrics.normalized_root_mse(GT_scaled[i,j,:,:,0], PD_scaled[i,j,:,:,0])\n",
    "    psnr[i][j] = metrics.peak_signal_noise_ratio(GT_scaled[i,j,:,:,0], PD_scaled[i,j,:,:,0], data_range=None)\n",
    "    ssim[i][j] = metrics.structural_similarity(GT_scaled[i,j,:,:,0], PD_scaled[i,j,:,:,0], win_size=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280dc197-1e18-4e4a-a9d5-74ae4493a22e",
   "metadata": {},
   "source": [
    "#### For Individual Video Sequence (17 frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3acfd-cb99-4559-b094-5cfc51a82494",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Individual Video Sequence (17 frames)\n",
    "mse_vs = np.zeros((a))\n",
    "rmse_vs = np.zeros((a))\n",
    "psnr_vs = np.zeros((a))\n",
    "ssim_vs = np.zeros((a))\n",
    "\n",
    "for i in range(GT_scaled.shape[0]):\n",
    "  # for j in range(GT_scaled.shape[1]):\n",
    "  mse_vs[i] = metrics.mean_squared_error(GT_scaled[i,:,:,:,0], PD_scaled[i,:,:,:,0])\n",
    "  rmse_vs[i] = metrics.normalized_root_mse(GT_scaled[i,:,:,:,0], PD_scaled[i,:,:,:,0])\n",
    "  psnr_vs[i] = metrics.peak_signal_noise_ratio(GT_scaled[i,:,:,:,0], PD_scaled[i,:,:,:,0], data_range=None)\n",
    "  ssim_vs[i] = metrics.structural_similarity(GT_scaled[i,:,:,:,0], PD_scaled[i,:,:,:,0], win_size=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910b7ef-a93c-4fb6-8a70-be517d3d04d0",
   "metadata": {},
   "source": [
    "#### For ALL Validation set at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f3d06-aa8c-4b26-8888-8e1e31f6292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For ALL Validation set at once\n",
    "mse_all = metrics.mean_squared_error(GT_scaled, PD_scaled)\n",
    "rmse_all = metrics.normalized_root_mse(GT_scaled, PD_scaled)\n",
    "psnr_all = metrics.peak_signal_noise_ratio(GT_scaled, PD_scaled, data_range=None)\n",
    "ssim_all = metrics.structural_similarity(GT_scaled[:,:,:,:,0], PD_scaled[:,:,:,:,0], win_size=3)\n",
    "\n",
    "print(f'Average Mean Square Error of all the validation set is: {mse_all}')\n",
    "print(f'Average Root Mean Square Error of all the validation set is: {rmse_all}')\n",
    "print(f'Average Peak Signal to Noise Ratio of all the validation set is: {psnr_all}')\n",
    "print(f'Average Structural Simalirity Index of all the validation set is: {ssim_all}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2b179-b812-499e-b15c-5f0f40dc542e",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f790fa32-f485-4e30-a889-50c0aead182e",
   "metadata": {},
   "source": [
    "### For Individual Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab391f3-04f4-460e-8a03-ad18ab1e8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6.5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43fc77-9b47-42cf-81ef-9b3077365e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(mse.shape[1])+1\n",
    "\n",
    "for i in range(mse.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, mse[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"MSE of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"MSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2b276-4e81-4bcb-838e-ccf8be7fe997",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(rmse.shape[1])+1\n",
    "\n",
    "for i in range(rmse.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, rmse[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"RMSE of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"RMSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a27534-0c9f-4e75-8642-c26dd3cfa3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(psnr.shape[1])+1\n",
    "\n",
    "for i in range(psnr.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, psnr[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"PSNR of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"PSNR\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4306633-f7db-432d-95f3-55ceb1df7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(ssim.shape[1])+1\n",
    "\n",
    "for i in range(ssim.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, ssim[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"SSIM of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb36804-59df-48e1-aa57-fe0954561a82",
   "metadata": {},
   "source": [
    "### For Individual Video Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b30ad-87f2-4167-997c-ca30f8f7e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(mse_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, mse_vs, '-o', color='red')\n",
    "plt.title(\"MSE of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"MSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23cf1a-6c5b-4adf-9771-4bc4951486e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(rmse_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, rmse_vs, '-o', color='red')\n",
    "plt.title(\"RMSE of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"RMSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f4bac-9ad0-4a18-8d74-9e26c895f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(psnr_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, psnr_vs, '-o', color='red')\n",
    "plt.title(\"PSNR of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"PSNR\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09e542-72cb-4c39-867f-78cb82fad208",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(ssim_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, ssim_vs, '-o', color='red')\n",
    "plt.title(\"SSIM of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ba706-09bc-4297-86df-070f94e553fc",
   "metadata": {},
   "source": [
    "### 2 plots in 1 fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d89db0-9d7f-4808-983a-dc3d9cc24326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vs_no = np.arange(len(ssim_vs))+1\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Plot the first array on the left y-axis\n",
    "ax1.plot(vs_no, ssim_vs, '-o', color='tab:blue')\n",
    "ax1.set_xlabel(\"Video Sequences\", size=20)\n",
    "ax1.set_ylabel(\"SSIM\", size=20, color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', )\n",
    "# ax1.set_ylim([min(ssim_vs)*0.3, 1])  \n",
    "ax1.set_ylim([0.8, 1])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(6,))\n",
    "ax1.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "\n",
    "# Create a second y-axis on the right side\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the second array on the right y-axis\n",
    "ax2.plot(vs_no, rmse_vs, '-s', color='tab:red')\n",
    "ax2.set_ylabel(\"RMSE\", size=20, color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "# ax2.set_ylim([min(rmse_vs)*0.99, max(rmse_vs)*1.05])  \n",
    "ax2.set_ylim([0.05, 0.1])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax2.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "# Add labels to the plot\n",
    "# plt.title('SSIM & RMSE In all Video Seuences',size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/ssim_vs_rmse_G0.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa4472-caf3-469d-bf65-1ecc776ad939",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(len(mse_vs))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Plot the first array on the left y-axis\n",
    "ax1.plot(vs_no, mse_vs, '-o', color='tab:blue')\n",
    "ax1.set_xlabel(\"Video Sequences\", size=20)\n",
    "ax1.set_ylabel(\"MSE\", size=20, color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', )\n",
    "# ax1.set_ylim([min(mse_vs)*0.4, max(mse_vs)*1.05])  \n",
    "ax1.set_ylim([0, 120])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "ax1.yaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "\n",
    "# Create a second y-axis on the right side\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the second array on the right y-axis\n",
    "ax2.plot(vs_no, psnr_vs, '-s', color='tab:red')\n",
    "ax2.set_ylabel(\"PSNR\", size=20, color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "# ax2.set_ylim([min(psnr_vs)*0.95, max(psnr_vs)*1.2])  \n",
    "ax2.set_ylim([27, 45])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax2.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "# Add labels to the plot\n",
    "# plt.title('MSE & PSNR In all Video Seuences',size=20)\n",
    "# plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92133e5-7192-46f5-aeaa-077aa23e4c3b",
   "metadata": {},
   "source": [
    "# Generation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596f860-5b40-4bf6-8047-64c4d50ea091",
   "metadata": {},
   "source": [
    "## Method 0 \n",
    "## For Generation 1 prediction We also inherit Generation 0's predicted result as parent. \n",
    "## GEN One  [(2 PF + 1 PD$^{G_0}$)$_{frame}$  --> 1 PD$^{G_1}$$_{frame}$] \n",
    "(2 PF$_{frames}$ are from Phase Field Simulation & 1 PD$^{G_0}$$_{frame}$ is from Generation 0 predicted frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f0ed4-3f23-4bc3-ba8c-e6ba445bda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generation 1 of prediction\n",
    "## The new frame from prediction is added to the ending frame of X_val and first frame of X_val is removed. \n",
    "## i.e. X_val had f1, f2, f3 and predicted fp4. \n",
    "## Gen 1 input has f2, f3, fp4 and predicvts fp5\n",
    "\n",
    "X_g1 = np.delete(np.concatenate((X_val, PD_array), axis=-1), 0, axis=-1)\n",
    "X_g1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddef55-e19a-4c24-a620-95a134e99432",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We remove the last frame of every video sequence as now the 3 frames to predict next frame consists of gen0 prediction \n",
    "## i.e. we are using f18, f19, fp20 to predict fp21, but the problem here is we don't gave GT for fp21 \n",
    "## So, we remove the last frame of every video sequence hence we limit our predictions for 16 frames making the last input as f17, f18, fp19 to predict fp20\n",
    "\n",
    "X_g1 = np.delete(X_g1, -1, axis=1)\n",
    "X_g1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d36e7-bd15-48fc-a34e-fd6251e2b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We similarly remove the 1st frame of GT video sequence i.e. frame 4 of every video sequence\n",
    "## Because, we are using f2, f2, fp4 to predict fp5. So we need f5 of GT to quantify with hence we can remove f4 of GT has it was quantified in Generation 0\n",
    "y_g1 = np.delete(y_val, 0, axis=1)\n",
    "y_g1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aaae08-a54f-40a5-83e5-041048e3584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d, e = y_g1.shape[0], y_g1.shape[1], y_g1.shape[2], y_g1.shape[3], y_g1.shape[4] \n",
    "PD_G1 = np.zeros((a, b, c, d, e))   ## Predicted arrays\n",
    "\n",
    "for i in range(X_g1.shape[0]):\n",
    "  frames = X_g1[i]   ## taking 1 validation video sample at one time\n",
    "  PD_G1[i] = model.predict(np.expand_dims(frames, axis=0), verbose=0, workers=10, use_multiprocessing=True)\n",
    "\n",
    "y_g1.shape, PD_G1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c41897-2e9f-48c9-8c98-aa35eb3cf0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling the array values from 0 to 255 for 0 as black pixel and 255 as white pixel for calculation of structural similarity\n",
    "## Scaling is done to both Ground Truth and Predicted Frame\n",
    "def scale_array(arr):\n",
    "    arr_min = np.min(arr)  ## Flattens the array and finds the min value inside it\n",
    "    arr_max = np.max(arr)  ## Flattens the array and finds the max value inside it\n",
    "    scaled_array = (arr - arr_min) / (arr_max - arr_min) * 255\n",
    "    scaled_array = scaled_array.astype(np.uint8)\n",
    "    \n",
    "    return scaled_array\n",
    "\n",
    "y_g1_scaled = scale_array(y_g1)\n",
    "PD_G1_scaled = scale_array(PD_G1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3305bf4-7687-4fc2-94b9-d0fff83acb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = 30 # Video frame to look for visualtization\n",
    "\n",
    "fig, axes = plt.subplots(2, 16, figsize=(25, 4))\n",
    "fig.suptitle(f\"Generation 1 Prediction of Video Sequence No: {vs}\" , fontsize=20)\n",
    "for idx, ax in enumerate(axes[0]):\n",
    "    ax.imshow(np.squeeze(y_g1[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"GT Frame {idx + 5}\", size=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for idx, ax in enumerate(axes[1]):\n",
    "    ax.imshow(np.squeeze(PD_G1[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"Pred Frame {idx + 5}\", size=8)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a43600-31ea-4c6e-83c9-02ee051ae673",
   "metadata": {},
   "source": [
    "### Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457fbff9-5ec3-4393-ad95-963e0c3cd43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Individual Image Frames\n",
    "mse_g1 = np.zeros((a,b))\n",
    "rmse_g1 = np.zeros((a,b))\n",
    "psnr_g1 = np.zeros((a,b))\n",
    "ssim_g1 = np.zeros((a,b))\n",
    "\n",
    "for i in range(y_g1_scaled.shape[0]):\n",
    "  for j in range(y_g1_scaled.shape[1]):\n",
    "    mse_g1[i][j] = metrics.mean_squared_error(y_g1_scaled[i,j,:,:,0], PD_G1_scaled[i,j,:,:,0])\n",
    "    rmse_g1[i][j] = metrics.normalized_root_mse(y_g1_scaled[i,j,:,:,0], PD_G1_scaled[i,j,:,:,0])\n",
    "    psnr_g1[i][j] = metrics.peak_signal_noise_ratio(y_g1_scaled[i,j,:,:,0], PD_G1_scaled[i,j,:,:,0], data_range=None)\n",
    "    ssim_g1[i][j] = metrics.structural_similarity(y_g1_scaled[i,j,:,:,0], PD_G1_scaled[i,j,:,:,0], win_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3671493-0951-4982-a217-00b9e7f36943",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Individual Video Sequence (17 frames)\n",
    "mse_g1_vs = np.zeros((a))\n",
    "rmse_g1_vs = np.zeros((a))\n",
    "psnr_g1_vs = np.zeros((a))\n",
    "ssim_g1_vs = np.zeros((a))\n",
    "\n",
    "for i in range(y_g1_scaled.shape[0]):\n",
    "  # for j in range(y_g1_scaled.shape[1]):\n",
    "  mse_g1_vs[i] = metrics.mean_squared_error(y_g1_scaled[i,:,:,:,0], PD_G1_scaled[i,:,:,:,0])\n",
    "  rmse_g1_vs[i] = metrics.normalized_root_mse(y_g1_scaled[i,:,:,:,0], PD_G1_scaled[i,:,:,:,0])\n",
    "  psnr_g1_vs[i] = metrics.peak_signal_noise_ratio(y_g1_scaled[i,:,:,:,0], PD_G1_scaled[i,:,:,:,0], data_range=None)\n",
    "  ssim_g1_vs[i] = metrics.structural_similarity(y_g1_scaled[i,:,:,:,0], PD_G1_scaled[i,:,:,:,0], win_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e10b81-0ef2-4b25-b647-661ea0cb66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For ALL Validation set at once\n",
    "mse_g1_all = metrics.mean_squared_error(y_g1_scaled, PD_G1_scaled)\n",
    "rmse_g1_all = metrics.normalized_root_mse(y_g1_scaled, PD_G1_scaled)\n",
    "psnr_g1_all = metrics.peak_signal_noise_ratio(y_g1_scaled, PD_G1_scaled, data_range=None)\n",
    "ssim_g1_all = metrics.structural_similarity(y_g1_scaled[:,:,:,:,0], PD_G1_scaled[:,:,:,:,0], win_size=3)\n",
    "\n",
    "print(f'Average Mean Square Error of all the validation set is: {mse_g1_all}')\n",
    "print(f'Average Root Mean Square Error of all the validation set is: {rmse_g1_all}')\n",
    "print(f'Average Peak Signal to Noise Ratio of all the validation set is: {psnr_g1_all}')\n",
    "print(f'Average Structural Simalirity Index of all the validation set is: {ssim_g1_all}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c119a-6d6f-4ffc-898a-f073467b3f94",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792fdab-ba38-49b1-984f-ec86bb1dfafc",
   "metadata": {},
   "source": [
    "### For Individual Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac887c0d-7e7e-4809-a7d1-8d4eecbf212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(mse_g1.shape[1])+1\n",
    "\n",
    "for i in range(mse_g1.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, mse_g1[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"MSE of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"MSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03eeea4-fc77-4fa6-bac8-66b1e20d4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(rmse_g1.shape[1])+1\n",
    "\n",
    "for i in range(rmse_g1.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, rmse_g1[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"RMSE of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"RMSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3956ae-6c61-4b9e-9c53-394299632b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(psnr_g1.shape[1])+1\n",
    "\n",
    "for i in range(psnr_g1.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, psnr_g1[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"PSNR of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"PSNR\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ecc38-dacb-42c4-b94c-491a079996d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(ssim_g1.shape[1])+1\n",
    "\n",
    "for i in range(ssim_g1.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, ssim_g1[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"SSIM of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e38c8-ae53-48c5-9f0f-f5e04546cb99",
   "metadata": {},
   "source": [
    "### For Individual Video Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20657458-069b-426f-8923-5b4a8e794b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(mse_g1_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, mse_g1_vs, '-o', color='red')\n",
    "plt.title(\"MSE of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"MSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886cb61-a43e-45a1-9aa7-ea9b416d9863",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(rmse_g1_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, rmse_g1_vs, '-o', color='red')\n",
    "plt.title(\"RMSE of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"RMSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831e15b-fce7-4906-9074-833a415d4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(psnr_g1_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, psnr_g1_vs, '-o', color='red')\n",
    "plt.title(\"PSNR of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"PSNR\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592723db-b8a6-4481-8496-05c45c8ceff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(ssim_g1_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, ssim_g1_vs, '-o', color='red')\n",
    "plt.title(\"SSIM of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acab551-ab29-4415-9ade-e112014161a6",
   "metadata": {},
   "source": [
    "### 2 plots in 1 fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff5a2f-0cab-44b4-a393-10f98b734dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(len(ssim_g1_vs))+1\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Plot the first array on the left y-axis\n",
    "ax1.plot(vs_no, ssim_g1_vs, '-o', color='tab:blue')\n",
    "ax1.set_xlabel(\"Video Sequence No\", size=20)\n",
    "ax1.set_ylabel(\"SSIM\", size=20, color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', )\n",
    "# ax1.set_ylim([min(ssim_g1_vs)*0.3, 1])  \n",
    "ax1.set_ylim([0.75, 1])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(6,))\n",
    "ax1.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "\n",
    "# Create a second y-axis on the right side\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the second array on the right y-axis\n",
    "ax2.plot(vs_no, rmse_g1_vs, '-s', color='tab:red')\n",
    "ax2.set_ylabel(\"RMSE\", size=20, color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "# ax2.set_ylim([min(rmse_g1_vs)*0.99, max(rmse_g1_vs)*1.05]) \n",
    "ax2.set_ylim([0.07, 0.17])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Add labels to the plot\n",
    "# plt.title('SSIM & RMSE In all Video Seuences',size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('ssim_vs_rmse.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd07fa4-0fd8-4c2a-897c-fc376b69945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(len(mse_g1_vs))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Plot the first array on the left y-axis\n",
    "ax1.plot(vs_no, mse_g1_vs, '-o', color='tab:blue')\n",
    "ax1.set_xlabel(\"Video Sequence No\", size=20)\n",
    "ax1.set_ylabel(\"MSE\", size=20, color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', )\n",
    "# ax1.set_ylim([min(mse_g1_vs)*0.8, max(mse_g1_vs)*1.02]) \n",
    "ax1.set_ylim([0, 270])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "ax1.yaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "\n",
    "# Create a second y-axis on the right side\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the second array on the right y-axis\n",
    "ax2.plot(vs_no, psnr_g1_vs, '-s', color='tab:red')\n",
    "ax2.set_ylabel(\"PSNR\", size=20, color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "# ax2.set_ylim([min(psnr_g1_vs)*0.99, max(psnr_g1_vs)*1.05])  \n",
    "ax2.set_ylim([23, 35])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax2.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "# Add labels to the plot\n",
    "# plt.title('MSE & PSNR In all Video Seuences',size=20)\n",
    "# plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fe662-5065-46d0-96a4-c40759a73b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a91bf045-90ea-4ae0-9403-ac0b6c4217ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3221163-5107-444a-a5c9-e2680e851981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916c8d3-d661-43ca-a226-b8921aeeb961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = X_val[0:1,:,:,:,:]\n",
    "# A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce398de7-a7e3-4105-a1cb-c4713441fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B = model.predict(A, verbose=0, workers=10, use_multiprocessing=True)\n",
    "# B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90325ffd-6447-4613-a1d4-a85285394a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = X_val[0:1,:,:,:,:]\n",
    "# C = np.zeros((1, 1, 64, 64, 1))\n",
    "\n",
    "# for i in range(X_val.shape[1]):\n",
    "#   B = model.predict(A, verbose=0, workers=10, use_multiprocessing=True)\n",
    "#   A = np.delete(np.concatenate((A,B), axis=-1), 0, axis=-1)\n",
    "#   C = np.concatenate((C,B), axis=1)\n",
    "  \n",
    "# C = np.delete(C,0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa62a8-0731-4397-9b84-fd1703f5c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "# fig.suptitle(f\"Generational Prediction \" , fontsize=14)\n",
    "\n",
    "# for idx, ax in enumerate(axes[0]):\n",
    "#     ax.imshow(np.squeeze(y_val[0:1,idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "#     ax.set_title(f\"Frame {idx + 3}\")\n",
    "#     ax.axis(\"off\")\n",
    "\n",
    "# for idx, ax in enumerate(axes[1]):\n",
    "#     ax.imshow(np.squeeze(C[0:1,idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "#     ax.set_title(f\"Frame {idx + 3}\")\n",
    "#     ax.axis(\"off\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f89388-62f4-4dd2-bd56-081bc690e684",
   "metadata": {},
   "source": [
    "# Generation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea224fa-4c98-40cc-8dde-c559637d05bf",
   "metadata": {},
   "source": [
    "## For Generation 2 prediction We also inherit Generation 0's and Generation 1's predicted result as parent. \n",
    "## GEN 2 ==> [(1 PF + 1 PD$^{G_0}$ + 1 PD$^{G_1}$)$_{frame}$  --> 1 PD$^{G_2}$$_{frame}$] \n",
    "(1 PF$_{frame}$ is from Phase Field Simulation & 1 PD$^{G_0}$$_{frame}$ is from Generation 0 predicted frame and 1 PD$^{G_1}$$_{frame}$ is from Generation 1 predicted frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2df0b9-9fdf-45c3-8265-478e3ae7fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generation 2 of prediction\n",
    "## The new frame from prediction is added to the ending frame of X_g1 and first frame of X_g1 is removed. \n",
    "## i.e. X_g1 had f2, f3, fp4 and predicted fp5. \n",
    "## Gen 2 input has f3, fp4, fp5 and predicvts fp6\n",
    "\n",
    "X_g2 = np.delete(np.concatenate((X_g1, PD_G1), axis=-1), 0, axis=-1)\n",
    "X_g2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf1946-28f0-4084-a53a-ff67713060fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We remove the last frame of every video sequence as now the 3 frames to predict next frame consists of gen0 prediction \n",
    "## i.e. we are using f18, f19, fp20 to predict fp21, but the problem here is we don't gave GT for fp21 \n",
    "## So, we remove the last frame of every video sequence hence we limit our predictions for 16 frames making the last input as f17, f18, fp19 to predict fp20\n",
    "\n",
    "X_g2 = np.delete(X_g2, -1, axis=1)\n",
    "X_g2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71889de8-2bf2-4269-98d1-ce19718b33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We similarly remove the 1st frame of GT video sequence i.e. frame 4 of every video sequence\n",
    "## Because, we are using f2, f2, fp4 to predict fp5. So we need f5 of GT to quantify with hence we can remove f4 of GT has it was quantified in Generation 0\n",
    "y_g2 = np.delete(y_g1, 0, axis=1)\n",
    "y_g2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da4eda8-74f8-46c7-b5ca-d3355c3acf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d, e = y_g2.shape[0], y_g2.shape[1], y_g2.shape[2], y_g2.shape[3], y_g2.shape[4] \n",
    "PD_G2 = np.zeros((a, b, c, d, e))   ## Predicted arrays\n",
    "\n",
    "for i in range(X_g1.shape[0]):\n",
    "  frames = X_g2[i]   ## taking 1 validation video sample at one time\n",
    "  PD_G2[i] = model.predict(np.expand_dims(frames, axis=0), verbose=0, workers=10, use_multiprocessing=True)\n",
    "\n",
    "y_g2.shape, PD_G2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94f8b7-8b57-443c-8268-3ee8390bd5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling the array values from 0 to 255 for 0 as black pixel and 255 as white pixel for calculation of structural similarity\n",
    "## Scaling is done to both Ground Truth and Predicted Frame\n",
    "def scale_array(arr):\n",
    "    arr_min = np.min(arr)  ## Flattens the array and finds the min value inside it\n",
    "    arr_max = np.max(arr)  ## Flattens the array and finds the max value inside it\n",
    "    scaled_array = (arr - arr_min) / (arr_max - arr_min) * 255\n",
    "    scaled_array = scaled_array.astype(np.uint8)\n",
    "    \n",
    "    return scaled_array\n",
    "\n",
    "y_g2_scaled = scale_array(y_g2)\n",
    "PD_G2_scaled = scale_array(PD_G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1adbf7-5f16-42a4-9ed8-cdc58156ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = 22 # Video frame to look for visualtization\n",
    "\n",
    "fig, axes = plt.subplots(2, 15, figsize=(25, 4))\n",
    "fig.suptitle(f\"Generation 1 Prediction of Video Sequence No: {vs}\" , fontsize=20)\n",
    "for idx, ax in enumerate(axes[0]):\n",
    "    ax.imshow(np.squeeze(y_g2[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"GT Frame {idx + 6}\", size=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for idx, ax in enumerate(axes[1]):\n",
    "    ax.imshow(np.squeeze(PD_G2[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"Pred Frame {idx + 6}\", size=8)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7efcd0-6fd1-438e-8cfc-2b1db92a8eef",
   "metadata": {},
   "source": [
    "### Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2458ae-eeef-4640-874d-ff5377c341b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Individual Image Frames\n",
    "mse_g2 = np.zeros((a,b))\n",
    "rmse_g2 = np.zeros((a,b))\n",
    "psnr_g2 = np.zeros((a,b))\n",
    "ssim_g2 = np.zeros((a,b))\n",
    "\n",
    "for i in range(y_g2_scaled.shape[0]):\n",
    "  for j in range(y_g2_scaled.shape[1]):\n",
    "    mse_g2[i][j] = metrics.mean_squared_error(y_g2_scaled[i,j,:,:,0], PD_G2_scaled[i,j,:,:,0])\n",
    "    rmse_g2[i][j] = metrics.normalized_root_mse(y_g2_scaled[i,j,:,:,0], PD_G2_scaled[i,j,:,:,0])\n",
    "    psnr_g2[i][j] = metrics.peak_signal_noise_ratio(y_g2_scaled[i,j,:,:,0], PD_G2_scaled[i,j,:,:,0], data_range=None)\n",
    "    ssim_g2[i][j] = metrics.structural_similarity(y_g2_scaled[i,j,:,:,0], PD_G2_scaled[i,j,:,:,0], win_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b3db1-219a-4b4b-a259-632b800170f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Individual Video Sequence (17 frames)\n",
    "mse_g2_vs = np.zeros((a))\n",
    "rmse_g2_vs = np.zeros((a))\n",
    "psnr_g2_vs = np.zeros((a))\n",
    "ssim_g2_vs = np.zeros((a))\n",
    "\n",
    "for i in range(y_g2_scaled.shape[0]):\n",
    "  # for j in range(y_g2_scaled.shape[1]):\n",
    "  mse_g2_vs[i] = metrics.mean_squared_error(y_g2_scaled[i,:,:,:,0], PD_G2_scaled[i,:,:,:,0])\n",
    "  rmse_g2_vs[i] = metrics.normalized_root_mse(y_g2_scaled[i,:,:,:,0], PD_G2_scaled[i,:,:,:,0])\n",
    "  psnr_g2_vs[i] = metrics.peak_signal_noise_ratio(y_g2_scaled[i,:,:,:,0], PD_G2_scaled[i,:,:,:,0], data_range=None)\n",
    "  ssim_g2_vs[i] = metrics.structural_similarity(y_g2_scaled[i,:,:,:,0], PD_G2_scaled[i,:,:,:,0], win_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c9f14-27e7-4581-97e4-ba0c86ceb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For ALL Validation set at once\n",
    "mse_g2_all = metrics.mean_squared_error(y_g2_scaled, PD_G2_scaled)\n",
    "rmse_g2_all = metrics.normalized_root_mse(y_g2_scaled, PD_G2_scaled)\n",
    "psnr_g2_all = metrics.peak_signal_noise_ratio(y_g2_scaled, PD_G2_scaled, data_range=None)\n",
    "ssim_g2_all = metrics.structural_similarity(y_g2_scaled[:,:,:,:,0], PD_G2_scaled[:,:,:,:,0], win_size=3)\n",
    "\n",
    "print(f'Average Mean Square Error of all the validation set is: {mse_g2_all}')\n",
    "print(f'Average Root Mean Square Error of all the validation set is: {rmse_g2_all}')\n",
    "print(f'Average Peak Signal to Noise Ratio of all the validation set is: {psnr_g2_all}')\n",
    "print(f'Average Structural Simalirity Index of all the validation set is: {ssim_g2_all}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc10dba-3f57-437e-b435-411163da03c1",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c113668c-640a-4992-9afd-8cc74ac1ff68",
   "metadata": {},
   "source": [
    "### For Individual Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060706b-bec7-4a00-b5d0-82d29b2c62c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(mse_g2.shape[1])+1\n",
    "\n",
    "for i in range(mse_g2.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, mse_g2[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"MSE of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"MSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae6333-c9fb-49a0-9878-b051cb11ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(rmse_g2.shape[1])+1\n",
    "\n",
    "for i in range(rmse_g2.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, rmse_g2[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"RMSE of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"RMSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08a746-a80e-4887-880f-8fec70ca9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(psnr_g2.shape[1])+1\n",
    "\n",
    "for i in range(psnr_g2.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, psnr_g2[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"PSNR of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"PSNR\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d975fed-5964-4695-9ec4-7c1806db8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(ssim_g2.shape[1])+1\n",
    "\n",
    "for i in range(ssim_g2.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, ssim_g2[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"SSIM of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad8e09-edea-41aa-9f5f-bc73ec75bfd9",
   "metadata": {},
   "source": [
    "### For Individual Video Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df96476-42b8-4ffb-8577-0248d81f3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(mse_g2_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, mse_g2_vs, '-o', color='red')\n",
    "plt.title(\"MSE of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"MSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a3d33-9b68-490a-b05b-6c44b367c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(rmse_g2_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, rmse_g2_vs, '-o', color='red')\n",
    "plt.title(\"RMSE of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"RMSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5a970-a03e-4bd2-ae4e-66fd1e03fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(psnr_g2_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, psnr_g2_vs, '-o', color='red')\n",
    "plt.title(\"PSNR of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"PSNR\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c7e27-3a62-4762-ad1c-f12e298e665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(ssim_g2_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, ssim_g2_vs, '-o', color='red')\n",
    "plt.title(\"SSIM of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda0eeba-89bd-4911-bdea-941b0b3c8cc6",
   "metadata": {},
   "source": [
    "### 2 plots in 1 fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a2522-6fe9-4cd1-a77d-ee7b84112143",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(len(ssim_g2_vs))+1\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Plot the first array on the left y-axis\n",
    "ax1.plot(vs_no, ssim_g2_vs, '-o', color='tab:blue')\n",
    "ax1.set_xlabel(\"Video Sequence No\", size=20)\n",
    "ax1.set_ylabel(\"SSIM\", size=20, color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', )\n",
    "# ax1.set_ylim([min(ssim_g2_vs)*0.2, 1])  \n",
    "ax1.set_ylim([0.7, 1])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(6,))\n",
    "ax1.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "\n",
    "# Create a second y-axis on the right side\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the second array on the right y-axis\n",
    "ax2.plot(vs_no, rmse_g2_vs, '-s', color='tab:red')\n",
    "ax2.set_ylabel(\"RMSE\", size=20, color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "# ax2.set_ylim([min(rmse_g2_vs)*0.95, max(rmse_g2_vs)*1.2]) \n",
    "ax2.set_ylim([0.088, 0.2])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Add labels to the plot\n",
    "# plt.title('SSIM & RMSE In all Video Seuences',size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('ssim_vs_rmse.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91ce1b-b3bd-4de5-8f22-1353ffdf7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(len(mse_g2_vs))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Plot the first array on the left y-axis\n",
    "ax1.plot(vs_no, mse_g2_vs, '-o', color='tab:blue')\n",
    "ax1.set_xlabel(\"Video Sequence No\", size=20)\n",
    "ax1.set_ylabel(\"MSE\", size=20, color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', )\n",
    "# ax1.set_ylim([min(mse_g2_vs)*0.95, max(mse_g2_vs)*1.01])  \n",
    "ax1.set_ylim([0, 400])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "ax1.yaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "\n",
    "# Create a second y-axis on the right side\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the second array on the right y-axis\n",
    "ax2.plot(vs_no, psnr_g2_vs, '-s', color='tab:red')\n",
    "ax2.set_ylabel(\"PSNR\", size=20, color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "# ax2.set_ylim([min(psnr_g2_vs)*0.995, max(psnr_g2_vs)*1.01])  \n",
    "ax2.set_ylim([21.5, 35])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax2.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "# Add labels to the plot\n",
    "# plt.title('MSE & PSNR In all Video Seuences',size=20)\n",
    "# plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4bc42d-96b7-4ea6-9809-12ac067a631d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Generation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2049ef0-83b6-46c9-9a2c-ee9b08ebdb2e",
   "metadata": {},
   "source": [
    "## For Generation 3 prediction We inherit Generation 0's, Generation 1's and , Generation 2's predicted result as parent. \n",
    "## GEN 3 ==> [(1 PD$^{G_0}$ + 1 PD$^{G_1}$ + 1 PD$^{G_2}$)$_{frame}$  --> 1 PD$^{G_3}$$_{frame}$] \n",
    "(1 PD$^{G_0}$$_{frame}$ is from Generation 0 predicted frame, 1 PD$^{G_1}$$_{frame}$ is from Generation 1 predicted frame and 1 PD$^{G_2}$$_{frame}$ is from Generation 2 predicted frame)\n",
    "\n",
    "At this point we are only inheriting Model Predicted frames as input. So no phase filed generated frame is being used at this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6bb321-933a-49a5-91ec-445b966c77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generation 2 of prediction\n",
    "## The new frame from prediction is added to the ending frame of X_g2 and first frame of X_g2 is removed. \n",
    "## i.e. X_g2 had f3, fp4, fp5 and predicted fp6. \n",
    "## Gen 2 input has fp4, fp5, fp6 and predicvts fp7\n",
    "\n",
    "X_g3 = np.delete(np.concatenate((X_g2, PD_G2), axis=-1), 0, axis=-1)\n",
    "X_g3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de7e51-24e4-4bc9-8b7a-9e9c739c8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We remove the last frame of every video sequence as now the 3 frames to predict next frame consists of gen0 prediction \n",
    "## i.e. we are using f18, f19, fp20 to predict fp21, but the problem here is we don't gave GT for fp21 \n",
    "## So, we remove the last frame of every video sequence hence we limit our predictions for 16 frames making the last input as f17, f18, fp19 to predict fp20\n",
    "\n",
    "X_g3 = np.delete(X_g3, -1, axis=1)\n",
    "X_g3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bf6a4-b97e-4c61-9394-be1df5a40e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We similarly remove the 1st frame of GT video sequence i.e. frame 4 of every video sequence\n",
    "## Because, we are using f2, f2, fp4 to predict fp5. So we need f5 of GT to quantify with hence we can remove f4 of GT has it was quantified in Generation 0\n",
    "y_g3 = np.delete(y_g2, 0, axis=1)\n",
    "y_g3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5776566b-64cf-49a6-970f-12f2df8704a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d, e = y_g3.shape[0], y_g3.shape[1], y_g3.shape[2], y_g3.shape[3], y_g3.shape[4] \n",
    "PD_G3 = np.zeros((a, b, c, d, e))   ## Predicted arrays\n",
    "\n",
    "for i in range(X_g2.shape[0]):\n",
    "  frames = X_g3[i]   ## taking 1 validation video sample at one time\n",
    "  PD_G3[i] = model.predict(np.expand_dims(frames, axis=0), verbose=0, workers=10, use_multiprocessing=True)\n",
    "\n",
    "y_g3.shape, PD_G3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541bebf5-bd96-49a4-a152-a18e9e3f1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling the array values from 0 to 255 for 0 as black pixel and 255 as white pixel for calculation of structural similarity\n",
    "## Scaling is done to both Ground Truth and Predicted Frame\n",
    "def scale_array(arr):\n",
    "    arr_min = np.min(arr)  ## Flattens the array and finds the min value inside it\n",
    "    arr_max = np.max(arr)  ## Flattens the array and finds the max value inside it\n",
    "    scaled_array = (arr - arr_min) / (arr_max - arr_min) * 255\n",
    "    scaled_array = scaled_array.astype(np.uint8)\n",
    "    \n",
    "    return scaled_array\n",
    "\n",
    "y_g3_scaled = scale_array(y_g3)\n",
    "PD_G3_scaled = scale_array(PD_G3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb59027-7a59-4da5-91f5-f1bcc1e04a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = 96 # Video frame to look for visualtization\n",
    "\n",
    "fig, axes = plt.subplots(2, 14, figsize=(25, 4))\n",
    "fig.suptitle(f\"Generation 1 Prediction of Video Sequence No: {vs}\" , fontsize=20)\n",
    "for idx, ax in enumerate(axes[0]):\n",
    "    ax.imshow(np.squeeze(y_g3[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"GT Frame {idx + 7}\", size=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for idx, ax in enumerate(axes[1]):\n",
    "    ax.imshow(np.squeeze(PD_G3[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"Pred Frame {idx + 7}\", size=8)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276f39b-b368-42ad-86e3-f411b4bb64cf",
   "metadata": {},
   "source": [
    "### Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147d390-f96e-498e-863b-56eb9c7a5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Individual Image Frames\n",
    "mse_g3 = np.zeros((a,b))\n",
    "rmse_g3 = np.zeros((a,b))\n",
    "psnr_g3 = np.zeros((a,b))\n",
    "ssim_g3 = np.zeros((a,b))\n",
    "\n",
    "for i in range(y_g3_scaled.shape[0]):\n",
    "  for j in range(y_g3_scaled.shape[1]):\n",
    "    mse_g3[i][j] = metrics.mean_squared_error(y_g3_scaled[i,j,:,:,0], PD_G3_scaled[i,j,:,:,0])\n",
    "    rmse_g3[i][j] = metrics.normalized_root_mse(y_g3_scaled[i,j,:,:,0], PD_G3_scaled[i,j,:,:,0])\n",
    "    psnr_g3[i][j] = metrics.peak_signal_noise_ratio(y_g3_scaled[i,j,:,:,0], PD_G3_scaled[i,j,:,:,0], data_range=None)\n",
    "    ssim_g3[i][j] = metrics.structural_similarity(y_g3_scaled[i,j,:,:,0], PD_G3_scaled[i,j,:,:,0], win_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e3f3b-9eff-470a-8212-d84b6bf81cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Individual Video Sequence (17 frames)\n",
    "mse_g3_vs = np.zeros((a))\n",
    "rmse_g3_vs = np.zeros((a))\n",
    "psnr_g3_vs = np.zeros((a))\n",
    "ssim_g3_vs = np.zeros((a))\n",
    "\n",
    "for i in range(y_g3_scaled.shape[0]):\n",
    "  # for j in range(y_g3_scaled.shape[1]):\n",
    "  mse_g3_vs[i] = metrics.mean_squared_error(y_g3_scaled[i,:,:,:,0], PD_G3_scaled[i,:,:,:,0])\n",
    "  rmse_g3_vs[i] = metrics.normalized_root_mse(y_g3_scaled[i,:,:,:,0], PD_G3_scaled[i,:,:,:,0])\n",
    "  psnr_g3_vs[i] = metrics.peak_signal_noise_ratio(y_g3_scaled[i,:,:,:,0], PD_G3_scaled[i,:,:,:,0], data_range=None)\n",
    "  ssim_g3_vs[i] = metrics.structural_similarity(y_g3_scaled[i,:,:,:,0], PD_G3_scaled[i,:,:,:,0], win_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e3cf7-4722-4a36-b3ed-9ec4f9a675f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For ALL Validation set at once\n",
    "mse_g3_all = metrics.mean_squared_error(y_g3_scaled, PD_G3_scaled)\n",
    "rmse_g3_all = metrics.normalized_root_mse(y_g3_scaled, PD_G3_scaled)\n",
    "psnr_g3_all = metrics.peak_signal_noise_ratio(y_g3_scaled, PD_G3_scaled, data_range=None)\n",
    "ssim_g3_all = metrics.structural_similarity(y_g3_scaled[:,:,:,:,0], PD_G3_scaled[:,:,:,:,0], win_size=3)\n",
    "\n",
    "print(f'Average Mean Square Error of all the validation set is: {mse_g3_all}')\n",
    "print(f'Average Root Mean Square Error of all the validation set is: {rmse_g3_all}')\n",
    "print(f'Average Peak Signal to Noise Ratio of all the validation set is: {psnr_g3_all}')\n",
    "print(f'Average Structural Simalirity Index of all the validation set is: {ssim_g3_all}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef1ec1-a0bb-4edf-a6d4-0a46aaba1652",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f7d9d8-8caa-4b77-9e5f-105ff22489e3",
   "metadata": {},
   "source": [
    "### For Individual Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e4fd0-abd7-424e-a55e-c5607da5fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(mse_g3.shape[1])+1\n",
    "\n",
    "for i in range(mse_g3.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, mse_g3[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"MSE of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"MSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b282f-2d27-4743-bd01-393ed1a2ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(rmse_g3.shape[1])+1\n",
    "\n",
    "for i in range(rmse_g3.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, rmse_g3[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"RMSE of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"RMSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68df41-3e4d-4e52-aece-7e46a36d44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(psnr_g3.shape[1])+1\n",
    "\n",
    "for i in range(psnr_g3.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, psnr_g3[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"PSNR of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"PSNR\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2aa316-826d-4506-8954-dc80a37e6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(ssim_g3.shape[1])+1\n",
    "\n",
    "for i in range(ssim_g3.shape[0]):   ## We are iteraing over all 137 video samples\n",
    "    plt.plot(frame_no, ssim_g3[i], '-s',)# label=f\"VS$_{i}$\")\n",
    "\n",
    "plt.title(\"SSIM of video samples\", size=20)\n",
    "plt.xlabel(\"Frame No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0a9e9-fd91-4ef0-abba-305d75379dbd",
   "metadata": {},
   "source": [
    "### For Individual Video Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c96e51-bc25-4312-824c-25f3f58918ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(mse_g3_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, mse_g3_vs, '-o', color='red')\n",
    "plt.title(\"MSE of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"MSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aaf01e-292e-469d-950e-9ac547dabec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(rmse_g3_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, rmse_g3_vs, '-o', color='red')\n",
    "plt.title(\"RMSE of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"RMSE\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33035525-fd62-4290-8012-8e92501b167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(psnr_g3_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, psnr_g3_vs, '-o', color='red')\n",
    "plt.title(\"PSNR of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"PSNR\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081876cc-471a-42e9-91cc-0f6abe169bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(ssim_g3_vs.shape[0])+1\n",
    "\n",
    "plt.plot(vs_no, ssim_g3_vs, '-o', color='red')\n",
    "plt.title(\"SSIM of video samples\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "# plt.legend(loc=(1.01,0.3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085a98c-d55d-4a15-9962-857f7a800694",
   "metadata": {},
   "source": [
    "### 2 plots in 1 fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219622aa-26bc-4d64-8a36-0e52b56a5f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(len(ssim_g3_vs))+1\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Plot the first array on the left y-axis\n",
    "ax1.plot(vs_no, ssim_g3_vs, '-o', color='tab:blue')\n",
    "ax1.set_xlabel(\"Video Sequence No\", size=20)\n",
    "ax1.set_ylabel(\"SSIM\", size=20, color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', )\n",
    "# ax1.set_ylim([min(ssim_g3_vs)*0.05, 1]) \n",
    "ax1.set_ylim([0.55, 1])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(6,))\n",
    "ax1.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "\n",
    "# Create a second y-axis on the right side\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the second array on the right y-axis\n",
    "ax2.plot(vs_no, rmse_g3_vs, '-s', color='tab:red')\n",
    "ax2.set_ylabel(\"RMSE\", size=20, color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "# ax2.set_ylim([min(rmse_g3_vs)*0.95, max(rmse_g3_vs)*1.25]) \n",
    "ax2.set_ylim([0.11, 0.26])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Add labels to the plot\n",
    "# plt.title('SSIM & RMSE In all Video Seuences',size=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/ssim_vs_rmse_gen3.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7083e85-aca8-4ea0-a45a-62427d8fc624",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_no = np.arange(len(mse_g3_vs))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "\n",
    "# Plot the first array on the left y-axis\n",
    "ax1.plot(vs_no, mse_g3_vs, '-o', color='tab:blue')\n",
    "ax1.set_xlabel(\"Video Sequence No\", size=20)\n",
    "ax1.set_ylabel(\"MSE\", size=20, color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', )\n",
    "# ax1.set_ylim([min(mse_g3_vs)*0.75, max(mse_g3_vs)*1.01])  \n",
    "ax1.set_ylim([0, 680])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "ax1.yaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "\n",
    "# Create a second y-axis on the right side\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the second array on the right y-axis\n",
    "ax2.plot(vs_no, psnr_g3_vs, '-s', color='tab:red')\n",
    "ax2.set_ylabel(\"PSNR\", size=20, color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "# ax2.set_ylim([min(psnr_g3_vs)*0.995, max(psnr_g3_vs)*1.1]) \n",
    "ax2.set_ylim([19.5, 35])\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "\n",
    "ax2.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "# Add labels to the plot\n",
    "# plt.title('MSE & PSNR In all Video Seuences',size=20)\n",
    "# plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ee29f-c44b-42c7-b983-fa55c2f634c4",
   "metadata": {},
   "source": [
    "# Generational Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60788931-74a9-4070-9771-ce8799844d17",
   "metadata": {},
   "source": [
    "## Gen 0 Vs Gen 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee919241-864d-43a5-b842-f2070b17deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(ssim_vs.shape[0])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "\n",
    "plt.plot(frame_no, ssim_vs, '-o', color='red', label='Generation 0')\n",
    "plt.plot(frame_no, ssim_g1_vs,'--s', color='green', label='Generation 1')\n",
    "\n",
    "plt.title(\"SSIM: 'Gen 0' Vs 'Gen 1'\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "plt.legend(loc=3)\n",
    "plt.ylim([0.8,1.0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1c908-22ab-4990-a357-bcebf967781c",
   "metadata": {},
   "source": [
    "## Gen 1 Vs Gen 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e74b7-de96-4d99-988c-3388d8804090",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(ssim_vs.shape[0])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "\n",
    "plt.plot(frame_no, ssim_g1_vs, '-s', color='green', label='Generation 1')\n",
    "plt.plot(frame_no, ssim_g2_vs,'--d', color='blue', label='Generation 2')\n",
    "\n",
    "\n",
    "plt.title(\"SSIM: 'Gen 1' Vs 'Gen 2'\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "plt.legend(loc=3)\n",
    "plt.ylim([0.8,1.0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469baef5-c06a-46bc-9bad-92c10edc6b03",
   "metadata": {},
   "source": [
    "## Gen 2 Vs Gen 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846c5e3-3535-4e32-8492-7487f57d7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_g2_vs.shape, ssim_g3_vs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41010698-6a93-4ab4-b5d3-aa9cfdb50f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(ssim_vs.shape[0])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "\n",
    "plt.plot(frame_no, ssim_g2_vs,'--d', color='blue', label='Generation 2')\n",
    "plt.plot(frame_no, ssim_g3_vs, ':x', color='m', label='Generation 3')\n",
    "\n",
    "plt.title(\"SSIM: 'Gen 1' Vs 'Gen 2'\", size=20)\n",
    "plt.xlabel(\"Video Sequence No\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "plt.legend(loc=3)\n",
    "plt.ylim([0.8,1.0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c444a-04d9-4d04-bfbf-367d4a752512",
   "metadata": {},
   "source": [
    "## Gen 0 Vs Gen 1 Vs Gen 2 Vs Gen 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b13058-1c77-4cdf-afab-66cf35d6f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_no = np.arange(ssim_vs.shape[0])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "\n",
    "plt.plot(frame_no, ssim_vs, '-o', color='red', label='Generation 0')\n",
    "plt.plot(frame_no, ssim_g1_vs, '-s', color='green', label='Generation 1')\n",
    "plt.plot(frame_no, ssim_g2_vs,'--d', color='blue', label='Generation 2')\n",
    "plt.plot(frame_no, ssim_g3_vs, ':x', color='m', label='Generation 3')\n",
    "\n",
    "\n",
    "# plt.title(\"SSIM: 'Gen 0' Vs 'Gen 1' Vs 'Gen 2' Vs 'Gen 3'\", size=20)\n",
    "plt.xlabel(\"Video Sequences\", size=20)\n",
    "plt.ylabel(\"SSIM\", size=20)\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "plt.legend()\n",
    "plt.ylim([0.8,1.0])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figures/SSIM_Gen_0_to_3.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3253fe-33b8-413e-9aad-6d216f920a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "132dc523-15dd-4dff-9c74-54fb34edca1f",
   "metadata": {},
   "source": [
    "# SkImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c559c0-7b4c-4d34-aa0a-4ed86667240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40518b1-8b8f-4bcb-9f8c-fc732be8e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_sam, _, PD_sam, _ = train_test_split(GT_array, PD_array, test_size=0.92)  ## Randomly sampling 10 video sequence for further anaylisis from Generation 0\n",
    "GT_sam.shape, PD_sam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef1803-6892-4ac9-ad54-377958a78b25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vs = 0 # Video frame to look for visualtization\n",
    "for vs in range(GT_sam.shape[0]):\n",
    "  fig, axes = plt.subplots(2, 17, figsize=(28, 4))\n",
    "  fig.suptitle(f\"Generation 1 Prediction of Video Sequence No: {vs}\" , fontsize=20)\n",
    "  for idx, ax in enumerate(axes[0]):\n",
    "      ax.imshow(np.squeeze(GT_sam[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "      ax.set_title(f\"GT Frame {idx + 4}\", size=10)\n",
    "      ax.axis(\"off\")\n",
    "  \n",
    "  for idx, ax in enumerate(axes[1]):\n",
    "      ax.imshow(np.squeeze(PD_sam[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "      ax.set_title(f\"Pred Frame {idx + 4}\", size=8)\n",
    "      ax.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0ecb8-ba7d-45a0-b10e-902b38148ebf",
   "metadata": {},
   "source": [
    "### Struct Simalirity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e46d7-d4df-4ecd-8cae-50c617548efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_sam_scaled = scale_array(GT_sam)\n",
    "PD_sam_scaled = scale_array(PD_sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc49185-4c3d-4683-abd0-45c55589bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_sim, diff = metrics.structural_similarity(GT_sam_scaled[:,:,:,:,0], PD_sam_scaled[:,:,:,:,0], win_size=3, full=True)\n",
    "# diff = (diff * 255).astype(\"uint8\")\n",
    "diff = scale_array(diff)\n",
    "diff = np.expand_dims(diff, axis=-1)\n",
    "print(f'Structural Similarity Index: {str_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721fab9-a992-4d2f-841d-585b75e41ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = 9\n",
    "fig, axes = plt.subplots(3, 4, figsize=(10, 8))\n",
    "fig.suptitle(f\"SSIM in Video of Sequence No: {vs}\" , fontsize=20)\n",
    "for idx, ax in enumerate(axes[0]):\n",
    "    ax.imshow(np.squeeze(GT_sam_scaled[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"GT Frame {idx + 4}\", size=14)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for idx, ax in enumerate(axes[1]):\n",
    "    ax.imshow(np.squeeze(PD_sam_scaled[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"Pred Frame {idx + 4}\", size=14)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for idx, ax in enumerate(axes[2]):\n",
    "    ax.imshow(np.squeeze(diff[vs:vs+1, idx:idx+1,:,:,:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"Diff in frame {idx + 4}\", size=14)\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figures/SSIM_Difference.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213f1d3-8dfb-4e75-b774-80fd3910efdb",
   "metadata": {},
   "source": [
    "### Blobs Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac345a-395e-4eaa-a582-46ea6f7b4d61",
   "metadata": {},
   "source": [
    "https://github.com/amirdizche/BlobDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb96745-03ea-4761-9a58-aea54996b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_blob.html#sphx-glr-auto-examples-features-detection-plot-blob-py\n",
    "# names = ['Laplacian of Gaussian', 'Difference of Gaussian', 'Determinant of Hessian']\n",
    "from math import sqrt\n",
    "from skimage import data\n",
    "from skimage.feature import blob_dog, blob_log, blob_doh\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c0b8e-23da-4349-8080-88751d2559c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Blobs Counting\n",
    "\n",
    "LoG_GT, LoG_PD = [], []\n",
    "DoG_GT, DoG_PD = [], []\n",
    "ratio_LoG, ratio_DoG = [], []   ## It is the ratio of no ob blobs detected in Predicted image and Blobs detected in Ground Truth\n",
    "\n",
    "for vs in range(GT_sam_scaled.shape[0]):\n",
    "  for f in range(GT_sam_scaled.shape[1]):\n",
    "    img_gt = GT_sam_scaled[vs, f,:,:,0]\n",
    "    img_pd = PD_sam_scaled[vs, f,:,:,0]\n",
    "    \n",
    "    log_gt = blob_log(img_gt, max_sigma=30, num_sigma=10, threshold=.1)\n",
    "    log_pd = blob_log(img_pd, max_sigma=30, num_sigma=10, threshold=.1)\n",
    "    dog_gt = blob_dog(img_gt, max_sigma=30, threshold=.1)\n",
    "    dog_pd = blob_dog(img_pd, max_sigma=30, threshold=.1)\n",
    "\n",
    "    LoG_GT.append(log_gt.shape[0]) ## Counting no of blobs for LoG in Ground Truth Image\n",
    "    LoG_PD.append(log_pd.shape[0]) ## Counting no of blobs for LoG in Predicted Image\n",
    "    DoG_GT.append(dog_gt.shape[0]) ## Counting no of blobs for DoG in Ground Truth Image\n",
    "    DoG_PD.append(dog_pd.shape[0]) ## Counting no of blobs for DoG in Predicted Image\n",
    "    ratio_LoG.append(round(log_pd.shape[0]/log_gt.shape[0] ,2))\n",
    "    ratio_DoG.append(round(dog_pd.shape[0]/dog_gt.shape[0] ,2))\n",
    "\n",
    "print(f'Average LoG Blob detection ratio in predicted to ground truth image is : {np.average(ratio_LoG)}')\n",
    "print(f'Average DoG Blob detection ratio in predicted to ground truth image is : {np.average(ratio_DoG)}')\n",
    "\n",
    "print(f'Standard Deviation of the LoG No of Blob Detected in PD:GT => {np.std(ratio_LoG)}')\n",
    "print(f'Standard Deviation of the DoG No of Blob Detected in PD:GT => {np.std(ratio_DoG)}')\n",
    "\n",
    "print(f'Variance in the LoG No of Blob Detected in PD:GT => {np.var(ratio_LoG)}')\n",
    "print(f'Variance in the DoG No of Blob Detected in PD:GT => {np.var(ratio_DoG)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb468e02-054d-457e-adeb-4717ae945954",
   "metadata": {},
   "outputs": [],
   "source": [
    "## laplacian is more noise sensetive\n",
    "x_ = range(len(ratio_LoG))\n",
    "\n",
    "mean = np.mean(ratio_LoG)\n",
    "std = np.std(ratio_LoG)\n",
    "var = np.var(ratio_LoG)\n",
    "\n",
    "plt.scatter(x_, ratio_LoG)\n",
    "plt.plot(x_, ratio_LoG)\n",
    "\n",
    "plt.axhline(mean, color='r', linestyle='--', label=f'mean = {round(mean,2)}')\n",
    "plt.axhline(mean+std, color='g', linestyle='--', label='$\\pm$'+ f'std = {round(std,4)}')\n",
    "plt.axhline(mean-std, color='g', linestyle='--')\n",
    "\n",
    "plt.fill_between(x_, mean-var, mean+var, color='m', alpha=0.5, label=f'variance = {round(var,4)}')\n",
    "\n",
    "plt.title(\"Blob Detection Ratio (Laplacian of Gaussian), Standard Deviation, and its Variance\", size=20)\n",
    "plt.xlabel(\"Image Frames\", size=20)\n",
    "plt.ylabel(\"Ratio of Blobs Detected: \"+ r\"$\\frac{PD}{GT}$\", size=20)\n",
    "plt.legend(loc=4, fontsize=15)\n",
    "\n",
    "plt.text(0,0.75, \"PD: Predicted Image \\nGT: Ground Truth Image\", size = 20)\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/LoG_Blob_detected.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3058515-aa23-496c-aded-1f748a6624ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = range(len(ratio_DoG))\n",
    "\n",
    "mean = np.mean(ratio_DoG)\n",
    "std = np.std(ratio_DoG)\n",
    "var = np.var(ratio_DoG)\n",
    "\n",
    "plt.scatter(x_, ratio_DoG)\n",
    "plt.plot(x_, ratio_DoG, color='black')\n",
    "\n",
    "plt.axhline(mean, color='r', linestyle='--', label=f'mean = {round(mean,2)}')\n",
    "plt.axhline(mean+std, color='r', linestyle='--', label='$\\pm$'+ f'std = {round(std,4)}')\n",
    "plt.axhline(mean-std, color='r', linestyle='--')\n",
    "\n",
    "plt.fill_between(x_, mean-var, mean+var, color='m', alpha=0.5, label=f'variance = {round(var,4)}' )\n",
    "\n",
    "plt.title(\"Blob Detection Ratio (Difference of Gaussian), Standard Deviation, and its Variance\", size=20)\n",
    "plt.xlabel(\"Image Frames\", size=20)\n",
    "plt.ylabel(\"Ratio of Blobs Detected: \"+ r\"$\\frac{PD}{GT}$\", size=20)\n",
    "plt.legend(loc=4, fontsize=15)\n",
    "\n",
    "plt.text(0,0.85, \"PD: Predicted Image \\nGT: Ground Truth Image\", size = 20)\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/DoG_Blob_detected.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb064e-56f2-484f-877f-26b69d9f8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = 6  ## Video sequence no\n",
    "f = 5  ## Frame no\n",
    "\n",
    "img_gt = GT_sam_scaled[vs, f,:,:,0]\n",
    "img_pd = PD_sam_scaled[vs, f,:,:,0]\n",
    "\n",
    "log_gt = blob_log(img_gt, max_sigma=30, num_sigma=10, threshold=.1)\n",
    "log_pd = blob_log(img_pd, max_sigma=30, num_sigma=10, threshold=.1)\n",
    "log_gt[:, 2] = log_gt[:, 2] * sqrt(2)   # # Compute radii in the 3rd column.\n",
    "log_pd[:, 2] = log_pd[:, 2] * sqrt(2)\n",
    "\n",
    "dog_gt = blob_dog(img_gt, max_sigma=30, threshold=.1)\n",
    "dog_pd = blob_dog(img_pd, max_sigma=30, threshold=.1)\n",
    "dog_gt[:, 2] = dog_gt[:, 2] * sqrt(2)   # # Compute radii in the 3rd column.\n",
    "dog_pd[:, 2] = dog_pd[:, 2] * sqrt(2)\n",
    "\n",
    "blobs_list = [log_gt, dog_gt, log_pd, dog_pd]\n",
    "colors = ['m', 'm', 'r', 'r']\n",
    "\n",
    "titles = [ 'LoG ', 'DoG ', 'LoG ','DoG ',]  # names = ['Laplacian of Gaussian', 'Difference of Gaussian', 'Determinant of Hessian']\n",
    "# titles = [\"Ground Truth\", \"Predicted\", \"Ground Truth\", \"Predicted\"]\n",
    "\n",
    "sequence = zip(blobs_list, colors, titles)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9, 9), sharex=True, sharey=True)\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for idx, (blobs, color, title) in enumerate(sequence):\n",
    "  # ax[idx].set_title(title+f\"\\nFrame: {f} of Video {vs}\", size = 20)\n",
    "  ax[idx].set_xticks([])\n",
    "  ax[idx].set_yticks([])\n",
    "\n",
    "  if idx < 2:\n",
    "    ax[idx].imshow(img_gt, 'gray')\n",
    "    ax[0].set_ylabel('Ground Truth Frames', size=20)\n",
    "    ax[idx].set_title(\"Blobs Counting using: \"+title+f\"\\nFrame: {f} of Video {vs}\", size = 20)\n",
    "\n",
    "  else:\n",
    "    ax[idx].imshow(img_pd, 'gray')\n",
    "    ax[2].set_ylabel('Predicted Frames', size=20)\n",
    "\n",
    "  for blob in blobs:\n",
    "        y, x, r = blob\n",
    "        c = plt.Circle((x, y), r, color=color, linewidth=2, linestyle='--', fill=False)\n",
    "        ax[idx].add_patch(c)\n",
    "  # ax[idx].set_axis_off()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figures/Blobs_counting.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4bca1-fa10-4980-a76b-b304a245e670",
   "metadata": {},
   "source": [
    "### Shape Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abff8d-7d9c-4d95-ad23-8a136724765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "from skimage.feature import shape_index\n",
    "from skimage.draw import disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa57a1-4972-42d4-a445-718cfa2a1371",
   "metadata": {},
   "source": [
    "[1] https://www.sciencedirect.com/science/article/abs/pii/026288569290076F?via%3Dihub\n",
    "\n",
    "[2] https://journals.aps.org/prb/abstract/10.1103/PhysRevB.78.024113\n",
    "\n",
    "The shape index is a single valued measure of local curvature of image microstructure and it is mathematically obtained from the eigen values of the Hessian [1], defined by Koenderink & van Doorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a537f-d855-40a7-b9d6-4075d473c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shape Index\n",
    "## https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_shape_index.html#sphx-glr-auto-examples-features-detection-plot-shape-index-py\n",
    "\n",
    "vs = 2\n",
    "f = 11\n",
    "pd_img = PD_sam[vs,f,:,:,0]\n",
    "gt_img = GT_sam[vs,f,:,:,0]\n",
    "\n",
    "#############################################################################\n",
    "s = shape_index(gt_img)\n",
    "\n",
    "# In this example we want to detect 'spherical caps',\n",
    "# so we threshold the shape index map to\n",
    "# find points which are 'spherical caps' (~1)\n",
    "\n",
    "target = 1\n",
    "delta = 0.05\n",
    "\n",
    "point_y, point_x = np.where(np.abs(s - target) < delta)\n",
    "point_z = gt_img[point_y, point_x]\n",
    "\n",
    "# The shape index map relentlessly produces the shape, even that of noise.\n",
    "# In order to reduce the impact of noise, we apply a Gaussian filter to it,\n",
    "# and show the results once in\n",
    "\n",
    "s_smooth = ndi.gaussian_filter(s, sigma=0.5)\n",
    "\n",
    "point_y_s, point_x_s = np.where(np.abs(s_smooth - target) < delta)\n",
    "point_z_s = gt_img[point_y_s, point_x_s]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "\n",
    "ax1.imshow(gt_img, cmap=plt.cm.gray)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Ground Truth Image')\n",
    "scatter_settings = dict(alpha=0.75, s=10, linewidths=0)\n",
    "ax1.scatter(point_x, point_y, color='blue', **scatter_settings)\n",
    "ax1.scatter(point_x_s, point_y_s, color='green', **scatter_settings)\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2, projection='3d', sharex=ax1, sharey=ax1)\n",
    "x, y = np.meshgrid(np.arange(0, gt_img.shape[0], 1), np.arange(0, gt_img.shape[1], 1))\n",
    "ax2.plot_surface(x, y, gt_img, linewidth=0, alpha=0.5, color=\"green\")\n",
    "ax2.scatter(point_x, point_y, point_z, color='blue', label='$|s - 1|<0.05$', **scatter_settings)\n",
    "ax2.scatter(point_x_s, point_y_s, point_z_s, color='green', label='$|s\\' - 1|<0.05$', **scatter_settings)\n",
    "ax2.legend(loc='lower left')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('3D visualization')\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3, sharex=ax1, sharey=ax1)\n",
    "ax3.imshow(s, cmap=plt.cm.gray)\n",
    "ax3.axis('off')\n",
    "ax3.set_title(r'Shape index, $\\sigma=1$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('figures/shape_ind_GT.png')\n",
    "plt.show()\n",
    "\n",
    "#############################################################\n",
    "\n",
    "s = shape_index(pd_img)\n",
    "\n",
    "target = 1\n",
    "delta = 0.05\n",
    "\n",
    "point_y, point_x = np.where(np.abs(s - target) < delta)\n",
    "point_z = pd_img[point_y, point_x]\n",
    "\n",
    "# The shape index map relentlessly produces the shape, even that of noise.\n",
    "# In order to reduce the impact of noise, we apply a Gaussian filter to it,\n",
    "# and show the results once in\n",
    "\n",
    "s_smooth = ndi.gaussian_filter(s, sigma=0.5)\n",
    "\n",
    "point_y_s, point_x_s = np.where(np.abs(s_smooth - target) < delta)\n",
    "point_z_s = pd_img[point_y_s, point_x_s]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "\n",
    "ax1.imshow(pd_img, cmap=plt.cm.gray)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Predicted Image')\n",
    "scatter_settings = dict(alpha=0.75, s=10, linewidths=0)\n",
    "ax1.scatter(point_x, point_y, color='blue', **scatter_settings)\n",
    "ax1.scatter(point_x_s, point_y_s, color='green', **scatter_settings)\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2, projection='3d', sharex=ax1, sharey=ax1)\n",
    "x, y = np.meshgrid(np.arange(0, pd_img.shape[0], 1), np.arange(0, pd_img.shape[1], 1))\n",
    "ax2.plot_surface(x, y, pd_img, linewidth=0, alpha=0.5, color=\"red\")\n",
    "ax2.scatter(point_x, point_y, point_z, color='blue', label='$|s - 1|<0.05$', **scatter_settings)\n",
    "ax2.scatter(point_x_s, point_y_s, point_z_s, color='green', label='$|s\\' - 1|<0.05$', **scatter_settings)\n",
    "ax2.legend(loc='lower left')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('3D visualization')\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3, sharex=ax1, sharey=ax1)\n",
    "ax3.imshow(s, cmap=plt.cm.gray)\n",
    "ax3.axis('off')\n",
    "ax3.set_title(r'Shape index, $\\sigma=1$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('figures/shape_ind_PD.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00340c-5470-4da2-a677-1c1bfa578550",
   "metadata": {},
   "source": [
    "###  Edge-based and region-based segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ca13f-04af-47b4-a81d-fd66b9066eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = 2\n",
    "f = 11\n",
    "pd_img = scale_array(PD_sam[vs,f,:,:,0])\n",
    "gt_img = scale_array(GT_sam[vs,f,:,:,0])\n",
    "\n",
    "from skimage.exposure import histogram\n",
    "\n",
    "hist1, hist_centers1 = histogram(gt_img)\n",
    "hist2, hist_centers2 = histogram(pd_img)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8)) # adjust figure size as needed\n",
    "# create subplots\n",
    "\n",
    "ax1 = fig.add_subplot(2, 1, 1) # column 1 and 2, row 1\n",
    "ax2 = fig.add_subplot(2, 2, 3) # column 1, row 2\n",
    "ax3 = fig.add_subplot(2, 2, 4) # column 2, row 2\n",
    "\n",
    "ax1.plot(hist_centers1, hist1, lw=2, color='red', label='GT')\n",
    "ax1.plot(hist_centers2, hist2, lw=2, color=\"blue\", label='PD')\n",
    "ax1.set_title('histogram of gray values', size = 20)\n",
    "ax1.set_ylabel(\"No. of Pixels\", size= 20)\n",
    "ax1.set_xlabel(\"Pixel's Intensity Value (0 to 255)\", size= 20)\n",
    "ax1.legend(loc=0, fontsize=12)\n",
    "\n",
    "ax2.imshow(gt_img, cmap=plt.cm.gray)\n",
    "ax2.axis('off')\n",
    "ax2.set_title(f\"Ground Truth Frame {f} of VS {vs}  \" , size=12)\n",
    "\n",
    "ax3.imshow(pd_img, cmap=plt.cm.gray)\n",
    "ax3.axis('off')\n",
    "ax3.set_title(f\"Predicted Image Frame {f} of VS {vs} \", size=12)\n",
    "\n",
    "ax1.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "# display the plot\n",
    "\n",
    "# ax2.text(-20,73, \" The x-axis represents the pixel intensity values, while the y-axis represents the number of pixels that have that intensity value.\" )\n",
    "plt.savefig('figures/histogram.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b419b-3c35-4665-b1f2-597bee0941c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
    "\n",
    "# axes[0].imshow(img1>150 , cmap=plt.cm.gray)\n",
    "# axes[0].set_title('Original  > 150')\n",
    "\n",
    "# axes[1].imshow(img2 > 150, cmap=plt.cm.gray)\n",
    "# axes[1].set_title('Predicted > 150')\n",
    "\n",
    "# for a in axes:\n",
    "#     a.axis('off')\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9515c-7872-4971-8103-ad90ec2b152c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
